# Production ML CPU Deployment Optimization <!-- omit from toc -->

## Table of Contents <!-- omit from toc -->

- [Optimizing CPU Inference](#optimizing-cpu-inference)

## Optimizing CPU Inference

https://huggingface.co/blog/bert-cpu-scaling-part-1

https://huggingface.co/blog/bert-cpu-scaling-part-2

- More precisely, Ice Lake Xeon CPUs can achieve up to 75% faster inference on a variety of NLP tasks when comparing against the previous generation of Cascade Lake Xeon processors.
